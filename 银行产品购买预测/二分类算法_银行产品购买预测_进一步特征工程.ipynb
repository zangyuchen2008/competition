{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 客户购买预测\n",
    "https://www.kesci.com/home/competition/5c234c6626ba91002bfdfdd3/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取csv格式文件\n",
    "import pandas as pd\n",
    "path = './input/' #用户的路径环境\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "train = pd.read_csv(path+'train_set.csv')\n",
    "test = pd.read_csv(path+'test_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['y']=-1\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征自身计算groupby组合条件下的count特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_count(data, features):\n",
    "    feature_name = 'count'\n",
    "    for i in features:\n",
    "        feature_name += '_' + i\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: feature_name})\n",
    "    data = data.merge(temp, 'left', on=features)\n",
    "    return data,feature_name\n",
    "ll=[]\n",
    "for f in['campaign', 'contact','default','education','housing','job','loan','marital','poutcome','pdays','previous']:\n",
    "    data,_=feature_count(data,['day','month',f])\n",
    "    ll.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_count(df, df_feature,fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].count()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_count\" % (\"_\".join(fe[0]))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\")#.fillna(0)\n",
    "    return df,df_count.columns[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别和类别特征组合做count和unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n"
    }
   ],
   "source": [
    "sparse_feature= ['campaign','contact','default','education','housing','job','loan','marital','month','poutcome']\n",
    "dense_feature=['age', 'balance','day','duration','campaign','pdays','previous']\n",
    "crossf=[]\n",
    "for d in tqdm(sparse_feature):\n",
    "    for s in sparse_feature:\n",
    "        crossf+=['count_'+str(d)+'_'+str(s),'nunique_'+str(d)+'_'+str(s)]\n",
    "        temp=data.groupby(d)[s].agg(['count','nunique']).reset_index().rename(columns={'count':'count_'+str(d)+'_'+str(s),'nunique':'nunique_'+str(d)+'_'+str(s)})\n",
    "        data=pd.merge(data,temp,on=d,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别和类别交叉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in sparse_feature:\n",
    "#     for ii in sparse_feature:\n",
    "#         if i != ii:\n",
    "#             data[i + '_' + ii] = data[i].astype('str') + data[ii].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别和数值特征组合做各种最大最小标准差等统计值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10/10 [03:09<00:00, 18.96s/it]\n"
    }
   ],
   "source": [
    "for d in tqdm(sparse_feature):\n",
    "    for s in dense_feature:\n",
    "        crossf+=[str(d)+'_mean_'+str(s),str(d)+'_max_'+str(s),str(d)+'_min_'+str(s),str(d)+'_std_'+str(s)]\n",
    "        temp=data.groupby(d)[s].agg(['mean','max','min','std']).reset_index().rename(columns={'mean':str(d)+'_mean_'+str(s),'max':str(d)+'_max_'+str(s),'min':str(d)+'_min_'+str(s),'std':str(d)+'_std_'+str(s)})\n",
    "        data=pd.merge(data,temp,on=d,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID这里好像没用'ID',\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "a= list(data)\n",
    "a.remove('ID')\n",
    "a.remove('y')\n",
    "num_features= a\n",
    "\n",
    "\n",
    "one_hot_feature =['job', 'marital','education','default','housing','loan','contact','month','poutcome']\n",
    "\n",
    "\n",
    "data['new_con'] = data['job'].astype(str)\n",
    "for i in ['marital', 'education', 'contact','month','poutcome']:\n",
    "    data['new_con'] = data['new_con'].astype(str) + '_' + data[i].astype(str)\n",
    "data['new_con'] = data['new_con'].apply(lambda x: ' '.join(x.split('_')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in one_hot_feature:\n",
    "    try:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))\n",
    "    except:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_feature = ['new_con']\n",
    "train=data[data.y!=-1]\n",
    "# train, test, train_y, test_y = train_test_split(train,train_y,test_size=0.2, random_state=2018)\n",
    "test=data[data.y==-1]\n",
    "res=test[['ID']]\n",
    "# test=test.drop(['ID'],axis=1)\n",
    "train_x=train[num_features]\n",
    "test_x=test[num_features]\n",
    "enc = OneHotEncoder()\n",
    "for feature in one_hot_feature:\n",
    "    enc.fit(data[feature].values.reshape(-1, 1))\n",
    "    train_a=enc.transform(train[feature].values.reshape(-1, 1)).toarray()\n",
    "    test_a = enc.transform(test[feature].values.reshape(-1, 1)).toarray()\n",
    "    train_x= np.hstack((train_x, train_a))\n",
    "    test_x = np.hstack((test_x, test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[10]\ttraining's auc: 0.941475\tvalid_1's auc: 0.925542\n[11]\ttraining's auc: 0.94152\tvalid_1's auc: 0.925509\n[12]\ttraining's auc: 0.942639\tvalid_1's auc: 0.926901\n[13]\ttraining's auc: 0.942878\tvalid_1's auc: 0.927116\n[14]\ttraining's auc: 0.943118\tvalid_1's auc: 0.927558\n[15]\ttraining's auc: 0.943211\tvalid_1's auc: 0.927593\n[16]\ttraining's auc: 0.943663\tvalid_1's auc: 0.927821\n[17]\ttraining's auc: 0.944272\tvalid_1's auc: 0.928381\n[18]\ttraining's auc: 0.944436\tvalid_1's auc: 0.929095\n[19]\ttraining's auc: 0.944758\tvalid_1's auc: 0.929158\n[20]\ttraining's auc: 0.944836\tvalid_1's auc: 0.929288\n[21]\ttraining's auc: 0.945045\tvalid_1's auc: 0.929295\n[22]\ttraining's auc: 0.945392\tvalid_1's auc: 0.929472\n[23]\ttraining's auc: 0.945596\tvalid_1's auc: 0.929761\n[24]\ttraining's auc: 0.945657\tvalid_1's auc: 0.929775\n[25]\ttraining's auc: 0.945774\tvalid_1's auc: 0.929654\n[26]\ttraining's auc: 0.945804\tvalid_1's auc: 0.929608\n[27]\ttraining's auc: 0.946044\tvalid_1's auc: 0.929731\n[28]\ttraining's auc: 0.946165\tvalid_1's auc: 0.929846\n[29]\ttraining's auc: 0.946363\tvalid_1's auc: 0.93001\n[30]\ttraining's auc: 0.946617\tvalid_1's auc: 0.930173\n[31]\ttraining's auc: 0.946834\tvalid_1's auc: 0.930546\n[32]\ttraining's auc: 0.946981\tvalid_1's auc: 0.930517\n[33]\ttraining's auc: 0.94703\tvalid_1's auc: 0.930324\n[34]\ttraining's auc: 0.947355\tvalid_1's auc: 0.93043\n[35]\ttraining's auc: 0.947646\tvalid_1's auc: 0.930514\n[36]\ttraining's auc: 0.947823\tvalid_1's auc: 0.930584\n[37]\ttraining's auc: 0.948145\tvalid_1's auc: 0.930647\n[38]\ttraining's auc: 0.948268\tvalid_1's auc: 0.930749\n[39]\ttraining's auc: 0.948363\tvalid_1's auc: 0.930671\n[40]\ttraining's auc: 0.948391\tvalid_1's auc: 0.930533\n[41]\ttraining's auc: 0.948524\tvalid_1's auc: 0.930656\n[42]\ttraining's auc: 0.948662\tvalid_1's auc: 0.930643\n[43]\ttraining's auc: 0.948854\tvalid_1's auc: 0.930782\n[44]\ttraining's auc: 0.949147\tvalid_1's auc: 0.931022\n[45]\ttraining's auc: 0.949309\tvalid_1's auc: 0.931136\n[46]\ttraining's auc: 0.949413\tvalid_1's auc: 0.931164\n[47]\ttraining's auc: 0.949557\tvalid_1's auc: 0.931383\n[48]\ttraining's auc: 0.949679\tvalid_1's auc: 0.931471\n[49]\ttraining's auc: 0.94984\tvalid_1's auc: 0.931604\n[50]\ttraining's auc: 0.949911\tvalid_1's auc: 0.931669\n[51]\ttraining's auc: 0.950009\tvalid_1's auc: 0.931697\n[52]\ttraining's auc: 0.950112\tvalid_1's auc: 0.931639\n[53]\ttraining's auc: 0.950176\tvalid_1's auc: 0.931731\n[54]\ttraining's auc: 0.950303\tvalid_1's auc: 0.931806\n[55]\ttraining's auc: 0.950367\tvalid_1's auc: 0.931926\n[56]\ttraining's auc: 0.950431\tvalid_1's auc: 0.932001\n[57]\ttraining's auc: 0.950556\tvalid_1's auc: 0.932035\n[58]\ttraining's auc: 0.950716\tvalid_1's auc: 0.932032\n[59]\ttraining's auc: 0.950834\tvalid_1's auc: 0.932038\n[60]\ttraining's auc: 0.95094\tvalid_1's auc: 0.932144\n[61]\ttraining's auc: 0.951139\tvalid_1's auc: 0.932199\n[62]\ttraining's auc: 0.95124\tvalid_1's auc: 0.932326\n[63]\ttraining's auc: 0.951406\tvalid_1's auc: 0.932638\n[64]\ttraining's auc: 0.951524\tvalid_1's auc: 0.932626\n[65]\ttraining's auc: 0.951653\tvalid_1's auc: 0.932652\n[66]\ttraining's auc: 0.95172\tvalid_1's auc: 0.932614\n[67]\ttraining's auc: 0.9519\tvalid_1's auc: 0.932695\n[68]\ttraining's auc: 0.951983\tvalid_1's auc: 0.932744\n[69]\ttraining's auc: 0.952087\tvalid_1's auc: 0.932786\n[70]\ttraining's auc: 0.952262\tvalid_1's auc: 0.932896\n[71]\ttraining's auc: 0.952383\tvalid_1's auc: 0.932954\n[72]\ttraining's auc: 0.952458\tvalid_1's auc: 0.932911\n[73]\ttraining's auc: 0.952599\tvalid_1's auc: 0.932929\n[74]\ttraining's auc: 0.952733\tvalid_1's auc: 0.932914\n[75]\ttraining's auc: 0.95281\tvalid_1's auc: 0.932918\n[76]\ttraining's auc: 0.952979\tvalid_1's auc: 0.933044\n[77]\ttraining's auc: 0.953111\tvalid_1's auc: 0.933016\n[78]\ttraining's auc: 0.953429\tvalid_1's auc: 0.933179\n[79]\ttraining's auc: 0.953544\tvalid_1's auc: 0.933265\n[80]\ttraining's auc: 0.953648\tvalid_1's auc: 0.933212\n[81]\ttraining's auc: 0.953822\tvalid_1's auc: 0.933343\n[82]\ttraining's auc: 0.953919\tvalid_1's auc: 0.933351\n[83]\ttraining's auc: 0.95399\tvalid_1's auc: 0.933355\n[84]\ttraining's auc: 0.954094\tvalid_1's auc: 0.93338\n[85]\ttraining's auc: 0.954156\tvalid_1's auc: 0.933501\n[86]\ttraining's auc: 0.954273\tvalid_1's auc: 0.933504\n[87]\ttraining's auc: 0.954374\tvalid_1's auc: 0.933492\n[88]\ttraining's auc: 0.954504\tvalid_1's auc: 0.93359\n[89]\ttraining's auc: 0.954594\tvalid_1's auc: 0.933609\n[90]\ttraining's auc: 0.954781\tvalid_1's auc: 0.933696\n[91]\ttraining's auc: 0.954865\tvalid_1's auc: 0.933648\n[92]\ttraining's auc: 0.954953\tvalid_1's auc: 0.93362\n[93]\ttraining's auc: 0.955148\tvalid_1's auc: 0.933632\n[94]\ttraining's auc: 0.955229\tvalid_1's auc: 0.933687\n[95]\ttraining's auc: 0.955395\tvalid_1's auc: 0.933693\n[96]\ttraining's auc: 0.955471\tvalid_1's auc: 0.933677\n[97]\ttraining's auc: 0.955563\tvalid_1's auc: 0.933682\n[98]\ttraining's auc: 0.955639\tvalid_1's auc: 0.93369\n[99]\ttraining's auc: 0.955797\tvalid_1's auc: 0.933743\n[100]\ttraining's auc: 0.955893\tvalid_1's auc: 0.933722\n[101]\ttraining's auc: 0.955978\tvalid_1's auc: 0.933718\n[102]\ttraining's auc: 0.956099\tvalid_1's auc: 0.9337\n[103]\ttraining's auc: 0.956261\tvalid_1's auc: 0.933778\n[104]\ttraining's auc: 0.956383\tvalid_1's auc: 0.933844\n[105]\ttraining's auc: 0.95647\tvalid_1's auc: 0.933898\n[106]\ttraining's auc: 0.956557\tvalid_1's auc: 0.933899\n[107]\ttraining's auc: 0.956679\tvalid_1's auc: 0.933948\n[108]\ttraining's auc: 0.956803\tvalid_1's auc: 0.934017\n[109]\ttraining's auc: 0.95688\tvalid_1's auc: 0.933988\n[110]\ttraining's auc: 0.956994\tvalid_1's auc: 0.934086\n[111]\ttraining's auc: 0.957101\tvalid_1's auc: 0.934151\n[112]\ttraining's auc: 0.957175\tvalid_1's auc: 0.934234\n[113]\ttraining's auc: 0.95734\tvalid_1's auc: 0.934269\n[114]\ttraining's auc: 0.957446\tvalid_1's auc: 0.934236\n[115]\ttraining's auc: 0.957526\tvalid_1's auc: 0.934299\n[116]\ttraining's auc: 0.957625\tvalid_1's auc: 0.934342\n[117]\ttraining's auc: 0.957716\tvalid_1's auc: 0.934389\n[118]\ttraining's auc: 0.957817\tvalid_1's auc: 0.934417\n[119]\ttraining's auc: 0.957956\tvalid_1's auc: 0.934486\n[120]\ttraining's auc: 0.958065\tvalid_1's auc: 0.934527\n[121]\ttraining's auc: 0.958216\tvalid_1's auc: 0.934495\n[122]\ttraining's auc: 0.958296\tvalid_1's auc: 0.934563\n[123]\ttraining's auc: 0.95843\tvalid_1's auc: 0.934585\n[124]\ttraining's auc: 0.958504\tvalid_1's auc: 0.934663\n[125]\ttraining's auc: 0.958603\tvalid_1's auc: 0.934638\n[126]\ttraining's auc: 0.958691\tvalid_1's auc: 0.934624\n[127]\ttraining's auc: 0.958823\tvalid_1's auc: 0.934633\n[128]\ttraining's auc: 0.958927\tvalid_1's auc: 0.934627\n[129]\ttraining's auc: 0.95911\tvalid_1's auc: 0.93469\n[130]\ttraining's auc: 0.95923\tvalid_1's auc: 0.934675\n[131]\ttraining's auc: 0.959367\tvalid_1's auc: 0.934674\n[132]\ttraining's auc: 0.959462\tvalid_1's auc: 0.93471\n[133]\ttraining's auc: 0.959558\tvalid_1's auc: 0.934689\n[134]\ttraining's auc: 0.959636\tvalid_1's auc: 0.934755\n[135]\ttraining's auc: 0.959736\tvalid_1's auc: 0.934781\n[136]\ttraining's auc: 0.959892\tvalid_1's auc: 0.934819\n[137]\ttraining's auc: 0.960038\tvalid_1's auc: 0.934832\n[138]\ttraining's auc: 0.96013\tvalid_1's auc: 0.934833\n[139]\ttraining's auc: 0.960249\tvalid_1's auc: 0.934755\n[140]\ttraining's auc: 0.960374\tvalid_1's auc: 0.934801\n[141]\ttraining's auc: 0.960434\tvalid_1's auc: 0.934846\n[142]\ttraining's auc: 0.960526\tvalid_1's auc: 0.934851\n[143]\ttraining's auc: 0.960625\tvalid_1's auc: 0.934911\n[144]\ttraining's auc: 0.960746\tvalid_1's auc: 0.934931\n[145]\ttraining's auc: 0.960826\tvalid_1's auc: 0.934934\n[146]\ttraining's auc: 0.960951\tvalid_1's auc: 0.934969\n[147]\ttraining's auc: 0.961027\tvalid_1's auc: 0.934959\n[148]\ttraining's auc: 0.961137\tvalid_1's auc: 0.935009\n[149]\ttraining's auc: 0.961238\tvalid_1's auc: 0.935077\n[150]\ttraining's auc: 0.961341\tvalid_1's auc: 0.93512\n[151]\ttraining's auc: 0.961454\tvalid_1's auc: 0.935153\n[152]\ttraining's auc: 0.96161\tvalid_1's auc: 0.935114\n[153]\ttraining's auc: 0.961685\tvalid_1's auc: 0.935122\n[154]\ttraining's auc: 0.961773\tvalid_1's auc: 0.935199\n[155]\ttraining's auc: 0.961879\tvalid_1's auc: 0.935215\n[156]\ttraining's auc: 0.962004\tvalid_1's auc: 0.935241\n[157]\ttraining's auc: 0.962116\tvalid_1's auc: 0.935278\n[158]\ttraining's auc: 0.962209\tvalid_1's auc: 0.935254\n[159]\ttraining's auc: 0.962324\tvalid_1's auc: 0.935287\n[160]\ttraining's auc: 0.962473\tvalid_1's auc: 0.935243\n[161]\ttraining's auc: 0.962552\tvalid_1's auc: 0.935275\n[162]\ttraining's auc: 0.962658\tvalid_1's auc: 0.93527\n[163]\ttraining's auc: 0.962796\tvalid_1's auc: 0.935255\n[164]\ttraining's auc: 0.962899\tvalid_1's auc: 0.935248\n[165]\ttraining's auc: 0.963005\tvalid_1's auc: 0.935247\n[166]\ttraining's auc: 0.963117\tvalid_1's auc: 0.935283\n[167]\ttraining's auc: 0.963222\tvalid_1's auc: 0.935273\n[168]\ttraining's auc: 0.963311\tvalid_1's auc: 0.935294\n[169]\ttraining's auc: 0.963448\tvalid_1's auc: 0.935308\n[170]\ttraining's auc: 0.96354\tvalid_1's auc: 0.935335\n[171]\ttraining's auc: 0.963628\tvalid_1's auc: 0.935351\n[172]\ttraining's auc: 0.963734\tvalid_1's auc: 0.935317\n[173]\ttraining's auc: 0.963827\tvalid_1's auc: 0.935329\n[174]\ttraining's auc: 0.963895\tvalid_1's auc: 0.935337\n[175]\ttraining's auc: 0.96401\tvalid_1's auc: 0.935365\n[176]\ttraining's auc: 0.964087\tvalid_1's auc: 0.935366\n[177]\ttraining's auc: 0.964155\tvalid_1's auc: 0.935415\n[178]\ttraining's auc: 0.964284\tvalid_1's auc: 0.935376\n[179]\ttraining's auc: 0.964407\tvalid_1's auc: 0.935381\n[180]\ttraining's auc: 0.96451\tvalid_1's auc: 0.93541\n[181]\ttraining's auc: 0.964625\tvalid_1's auc: 0.935392\n[182]\ttraining's auc: 0.964723\tvalid_1's auc: 0.935375\n[183]\ttraining's auc: 0.964787\tvalid_1's auc: 0.935314\n[184]\ttraining's auc: 0.96488\tvalid_1's auc: 0.935349\n[185]\ttraining's auc: 0.96496\tvalid_1's auc: 0.935382\n[186]\ttraining's auc: 0.965023\tvalid_1's auc: 0.935406\n[187]\ttraining's auc: 0.965098\tvalid_1's auc: 0.935396\n[188]\ttraining's auc: 0.965203\tvalid_1's auc: 0.935409\n[189]\ttraining's auc: 0.96531\tvalid_1's auc: 0.935415\n[190]\ttraining's auc: 0.965387\tvalid_1's auc: 0.935453\n[191]\ttraining's auc: 0.965494\tvalid_1's auc: 0.935479\n[192]\ttraining's auc: 0.965564\tvalid_1's auc: 0.935484\n[193]\ttraining's auc: 0.965644\tvalid_1's auc: 0.93547\n[194]\ttraining's auc: 0.965732\tvalid_1's auc: 0.935481\n[195]\ttraining's auc: 0.965816\tvalid_1's auc: 0.935507\n[196]\ttraining's auc: 0.965903\tvalid_1's auc: 0.935476\n[197]\ttraining's auc: 0.966021\tvalid_1's auc: 0.935504\n[198]\ttraining's auc: 0.966088\tvalid_1's auc: 0.935561\n[199]\ttraining's auc: 0.966182\tvalid_1's auc: 0.935574\n[200]\ttraining's auc: 0.96627\tvalid_1's auc: 0.935592\n[201]\ttraining's auc: 0.966342\tvalid_1's auc: 0.935603\n[202]\ttraining's auc: 0.966431\tvalid_1's auc: 0.935547\n[203]\ttraining's auc: 0.966503\tvalid_1's auc: 0.935565\n[204]\ttraining's auc: 0.96656\tvalid_1's auc: 0.935572\n[205]\ttraining's auc: 0.966634\tvalid_1's auc: 0.935535\n[206]\ttraining's auc: 0.966712\tvalid_1's auc: 0.935514\n[207]\ttraining's auc: 0.966807\tvalid_1's auc: 0.93551\n[208]\ttraining's auc: 0.966887\tvalid_1's auc: 0.935563\n[209]\ttraining's auc: 0.966964\tvalid_1's auc: 0.935568\n[210]\ttraining's auc: 0.967041\tvalid_1's auc: 0.935585\n[211]\ttraining's auc: 0.967099\tvalid_1's auc: 0.935592\n[212]\ttraining's auc: 0.967173\tvalid_1's auc: 0.935593\n[213]\ttraining's auc: 0.967233\tvalid_1's auc: 0.935609\n[214]\ttraining's auc: 0.967328\tvalid_1's auc: 0.93562\n[215]\ttraining's auc: 0.96744\tvalid_1's auc: 0.935601\n[216]\ttraining's auc: 0.967505\tvalid_1's auc: 0.935569\n[217]\ttraining's auc: 0.967578\tvalid_1's auc: 0.935641\n[218]\ttraining's auc: 0.967694\tvalid_1's auc: 0.935615\n[219]\ttraining's auc: 0.967771\tvalid_1's auc: 0.935644\n[220]\ttraining's auc: 0.967896\tvalid_1's auc: 0.935642\n[221]\ttraining's auc: 0.967961\tvalid_1's auc: 0.93563\n[222]\ttraining's auc: 0.968021\tvalid_1's auc: 0.935639\n[223]\ttraining's auc: 0.968112\tvalid_1's auc: 0.935704\n[224]\ttraining's auc: 0.968207\tvalid_1's auc: 0.935742\n[225]\ttraining's auc: 0.968298\tvalid_1's auc: 0.935715\n[226]\ttraining's auc: 0.968371\tvalid_1's auc: 0.935693\n[227]\ttraining's auc: 0.968426\tvalid_1's auc: 0.935679\n[228]\ttraining's auc: 0.968513\tvalid_1's auc: 0.935657\n[229]\ttraining's auc: 0.968584\tvalid_1's auc: 0.935683\n[230]\ttraining's auc: 0.968658\tvalid_1's auc: 0.935716\n[231]\ttraining's auc: 0.968737\tvalid_1's auc: 0.935742\n[232]\ttraining's auc: 0.9688\tvalid_1's auc: 0.935761\n[233]\ttraining's auc: 0.968898\tvalid_1's auc: 0.935751\n[234]\ttraining's auc: 0.968994\tvalid_1's auc: 0.935749\n[235]\ttraining's auc: 0.969121\tvalid_1's auc: 0.935729\n[236]\ttraining's auc: 0.969192\tvalid_1's auc: 0.935698\n[237]\ttraining's auc: 0.969265\tvalid_1's auc: 0.935694\n[238]\ttraining's auc: 0.969356\tvalid_1's auc: 0.935695\n[239]\ttraining's auc: 0.969444\tvalid_1's auc: 0.935712\n[240]\ttraining's auc: 0.969514\tvalid_1's auc: 0.935726\n[241]\ttraining's auc: 0.969603\tvalid_1's auc: 0.935759\n[242]\ttraining's auc: 0.969701\tvalid_1's auc: 0.935775\n[243]\ttraining's auc: 0.96978\tvalid_1's auc: 0.935804\n[244]\ttraining's auc: 0.969859\tvalid_1's auc: 0.935768\n[245]\ttraining's auc: 0.969947\tvalid_1's auc: 0.93581\n[246]\ttraining's auc: 0.970024\tvalid_1's auc: 0.935812\n[247]\ttraining's auc: 0.970103\tvalid_1's auc: 0.935822\n[248]\ttraining's auc: 0.970165\tvalid_1's auc: 0.935832\n[249]\ttraining's auc: 0.970243\tvalid_1's auc: 0.935802\n[250]\ttraining's auc: 0.97033\tvalid_1's auc: 0.935775\n[251]\ttraining's auc: 0.970407\tvalid_1's auc: 0.93578\n[252]\ttraining's auc: 0.970481\tvalid_1's auc: 0.935769\n[253]\ttraining's auc: 0.970533\tvalid_1's auc: 0.935784\n[254]\ttraining's auc: 0.970601\tvalid_1's auc: 0.935788\n[255]\ttraining's auc: 0.970671\tvalid_1's auc: 0.935771\n[256]\ttraining's auc: 0.970751\tvalid_1's auc: 0.935784\n[257]\ttraining's auc: 0.970813\tvalid_1's auc: 0.935817\n[258]\ttraining's auc: 0.970906\tvalid_1's auc: 0.935797\n[259]\ttraining's auc: 0.970994\tvalid_1's auc: 0.935776\n[260]\ttraining's auc: 0.971071\tvalid_1's auc: 0.935788\n[261]\ttraining's auc: 0.971144\tvalid_1's auc: 0.935759\n[262]\ttraining's auc: 0.971237\tvalid_1's auc: 0.935783\n[263]\ttraining's auc: 0.971333\tvalid_1's auc: 0.935858\n[264]\ttraining's auc: 0.971416\tvalid_1's auc: 0.935847\n[265]\ttraining's auc: 0.971485\tvalid_1's auc: 0.935825\n[266]\ttraining's auc: 0.971548\tvalid_1's auc: 0.935807\n[267]\ttraining's auc: 0.971631\tvalid_1's auc: 0.935813\n[268]\ttraining's auc: 0.971707\tvalid_1's auc: 0.935808\n[269]\ttraining's auc: 0.971773\tvalid_1's auc: 0.935785\n[270]\ttraining's auc: 0.971847\tvalid_1's auc: 0.935818\n[271]\ttraining's auc: 0.97191\tvalid_1's auc: 0.935815\n[272]\ttraining's auc: 0.972\tvalid_1's auc: 0.935842\n[273]\ttraining's auc: 0.972059\tvalid_1's auc: 0.935834\n[274]\ttraining's auc: 0.972141\tvalid_1's auc: 0.935866\n[275]\ttraining's auc: 0.972203\tvalid_1's auc: 0.935838\n[276]\ttraining's auc: 0.972273\tvalid_1's auc: 0.93584\n[277]\ttraining's auc: 0.972354\tvalid_1's auc: 0.935787\n[278]\ttraining's auc: 0.972419\tvalid_1's auc: 0.93577\n[279]\ttraining's auc: 0.972493\tvalid_1's auc: 0.935723\n[280]\ttraining's auc: 0.972574\tvalid_1's auc: 0.935734\n[281]\ttraining's auc: 0.972641\tvalid_1's auc: 0.935725\n[282]\ttraining's auc: 0.972729\tvalid_1's auc: 0.935741\n[283]\ttraining's auc: 0.97278\tvalid_1's auc: 0.935722\n[284]\ttraining's auc: 0.972851\tvalid_1's auc: 0.935722\n[285]\ttraining's auc: 0.972918\tvalid_1's auc: 0.935697\n[286]\ttraining's auc: 0.972971\tvalid_1's auc: 0.935662\n[287]\ttraining's auc: 0.973056\tvalid_1's auc: 0.935645\n[288]\ttraining's auc: 0.973116\tvalid_1's auc: 0.935649\n[289]\ttraining's auc: 0.973169\tvalid_1's auc: 0.935643\n[290]\ttraining's auc: 0.97322\tvalid_1's auc: 0.93561\n[291]\ttraining's auc: 0.973288\tvalid_1's auc: 0.935624\n[292]\ttraining's auc: 0.97336\tvalid_1's auc: 0.935614\n[293]\ttraining's auc: 0.973437\tvalid_1's auc: 0.935609\n[294]\ttraining's auc: 0.973499\tvalid_1's auc: 0.935615\n[295]\ttraining's auc: 0.973565\tvalid_1's auc: 0.935595\n[296]\ttraining's auc: 0.973619\tvalid_1's auc: 0.935583\n[297]\ttraining's auc: 0.973684\tvalid_1's auc: 0.935589\n[298]\ttraining's auc: 0.973749\tvalid_1's auc: 0.935557\n[299]\ttraining's auc: 0.973807\tvalid_1's auc: 0.935552\n[300]\ttraining's auc: 0.973879\tvalid_1's auc: 0.935588\n[301]\ttraining's auc: 0.973972\tvalid_1's auc: 0.935585\n[302]\ttraining's auc: 0.974037\tvalid_1's auc: 0.935606\n[303]\ttraining's auc: 0.974092\tvalid_1's auc: 0.935637\n[304]\ttraining's auc: 0.974133\tvalid_1's auc: 0.935607\n[305]\ttraining's auc: 0.974236\tvalid_1's auc: 0.935619\n[306]\ttraining's auc: 0.974304\tvalid_1's auc: 0.935646\n[307]\ttraining's auc: 0.974364\tvalid_1's auc: 0.935657\n[308]\ttraining's auc: 0.974433\tvalid_1's auc: 0.935643\n[309]\ttraining's auc: 0.974494\tvalid_1's auc: 0.935645\n[310]\ttraining's auc: 0.97456\tvalid_1's auc: 0.935605\n[311]\ttraining's auc: 0.974633\tvalid_1's auc: 0.935608\n[312]\ttraining's auc: 0.974702\tvalid_1's auc: 0.935608\n[313]\ttraining's auc: 0.974754\tvalid_1's auc: 0.935612\n[314]\ttraining's auc: 0.974858\tvalid_1's auc: 0.935608\n[315]\ttraining's auc: 0.974926\tvalid_1's auc: 0.935606\n[316]\ttraining's auc: 0.975001\tvalid_1's auc: 0.935613\n[317]\ttraining's auc: 0.975067\tvalid_1's auc: 0.935597\n[318]\ttraining's auc: 0.97513\tvalid_1's auc: 0.935623\n[319]\ttraining's auc: 0.975207\tvalid_1's auc: 0.93565\n[320]\ttraining's auc: 0.97527\tvalid_1's auc: 0.935652\n[321]\ttraining's auc: 0.975337\tvalid_1's auc: 0.935675\n[322]\ttraining's auc: 0.975393\tvalid_1's auc: 0.935668\n[323]\ttraining's auc: 0.975437\tvalid_1's auc: 0.935682\n[324]\ttraining's auc: 0.97551\tvalid_1's auc: 0.935701\n[325]\ttraining's auc: 0.975572\tvalid_1's auc: 0.935688\n[326]\ttraining's auc: 0.975635\tvalid_1's auc: 0.935683\n[327]\ttraining's auc: 0.975695\tvalid_1's auc: 0.935701\n[328]\ttraining's auc: 0.975757\tvalid_1's auc: 0.935698\n[329]\ttraining's auc: 0.975821\tvalid_1's auc: 0.935681\n[330]\ttraining's auc: 0.975881\tvalid_1's auc: 0.935663\n[331]\ttraining's auc: 0.975951\tvalid_1's auc: 0.935647\n[332]\ttraining's auc: 0.976009\tvalid_1's auc: 0.93563\n[333]\ttraining's auc: 0.976055\tvalid_1's auc: 0.935612\n[334]\ttraining's auc: 0.976112\tvalid_1's auc: 0.935603\n[335]\ttraining's auc: 0.976173\tvalid_1's auc: 0.935628\n[336]\ttraining's auc: 0.976238\tvalid_1's auc: 0.935655\n[337]\ttraining's auc: 0.976307\tvalid_1's auc: 0.935629\n[338]\ttraining's auc: 0.976363\tvalid_1's auc: 0.935609\n[339]\ttraining's auc: 0.976447\tvalid_1's auc: 0.935666\n[340]\ttraining's auc: 0.976515\tvalid_1's auc: 0.935629\n[341]\ttraining's auc: 0.976553\tvalid_1's auc: 0.935627\n[342]\ttraining's auc: 0.976599\tvalid_1's auc: 0.935623\n[343]\ttraining's auc: 0.976655\tvalid_1's auc: 0.935595\n[344]\ttraining's auc: 0.976713\tvalid_1's auc: 0.935589\n[345]\ttraining's auc: 0.976756\tvalid_1's auc: 0.935586\n[346]\ttraining's auc: 0.976805\tvalid_1's auc: 0.935617\n[347]\ttraining's auc: 0.976867\tvalid_1's auc: 0.935606\n[348]\ttraining's auc: 0.976905\tvalid_1's auc: 0.935629\n[349]\ttraining's auc: 0.976951\tvalid_1's auc: 0.935609\n[350]\ttraining's auc: 0.977004\tvalid_1's auc: 0.93562\n[351]\ttraining's auc: 0.977067\tvalid_1's auc: 0.935653\n[352]\ttraining's auc: 0.97713\tvalid_1's auc: 0.935673\n[353]\ttraining's auc: 0.977191\tvalid_1's auc: 0.935723\n[354]\ttraining's auc: 0.977241\tvalid_1's auc: 0.935694\n[355]\ttraining's auc: 0.977285\tvalid_1's auc: 0.935714\n[356]\ttraining's auc: 0.977343\tvalid_1's auc: 0.935722\n[357]\ttraining's auc: 0.977398\tvalid_1's auc: 0.935705\n[358]\ttraining's auc: 0.97745\tvalid_1's auc: 0.935682\n[359]\ttraining's auc: 0.97751\tvalid_1's auc: 0.935687\n[360]\ttraining's auc: 0.977569\tvalid_1's auc: 0.935689\n[361]\ttraining's auc: 0.977629\tvalid_1's auc: 0.9357\n[362]\ttraining's auc: 0.9777\tvalid_1's auc: 0.935729\n[363]\ttraining's auc: 0.977775\tvalid_1's auc: 0.935737\n[364]\ttraining's auc: 0.977831\tvalid_1's auc: 0.935715\n[365]\ttraining's auc: 0.977894\tvalid_1's auc: 0.935673\n[366]\ttraining's auc: 0.977965\tvalid_1's auc: 0.935666\n[367]\ttraining's auc: 0.978025\tvalid_1's auc: 0.93564\n[368]\ttraining's auc: 0.978078\tvalid_1's auc: 0.935616\n[369]\ttraining's auc: 0.978133\tvalid_1's auc: 0.935605\n[370]\ttraining's auc: 0.978182\tvalid_1's auc: 0.935571\n[371]\ttraining's auc: 0.978235\tvalid_1's auc: 0.93557\n[372]\ttraining's auc: 0.978281\tvalid_1's auc: 0.935589\n[373]\ttraining's auc: 0.978342\tvalid_1's auc: 0.935598\n[374]\ttraining's auc: 0.978395\tvalid_1's auc: 0.935596\nEarly stopping, best iteration is:\n[274]\ttraining's auc: 0.972141\tvalid_1's auc: 0.935866\n"
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "for feature in vector_feature:\n",
    "    cv.fit(data[feature])\n",
    "    train_a = cv.transform(train[feature]).toarray()\n",
    "    test_a = cv.transform(test[feature]).toarray()\n",
    "    train_x = np.hstack((train_x, train_a))\n",
    "    test_x = np.hstack((test_x, test_a))\n",
    "\n",
    "# trainxx, vali_x, trainyy, vali_y = train_test_split(train_x,train_y,test_size=0.2, random_state=2018)\n",
    "n_splits=5\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "clf = lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\", num_leaves=30, reg_alpha=0, reg_lambda=0.,\n",
    "    max_depth=-1, n_estimators=1500, objective='binary',metric= 'auc',\n",
    "    subsample=0.95, colsample_bytree=0.7, subsample_freq=1,\n",
    "    learning_rate=0.02, random_state=2017\n",
    "    )\n",
    "\n",
    "train_y=train['y']\n",
    "res['pred'] = 0\n",
    "for train_idx, val_idx in kfold.split(train_x):\n",
    "    clf.random_state = clf.random_state + 1\n",
    "    train_x1 = train_x[train_idx]\n",
    "    train_y1 = train_y.loc[train_idx]\n",
    "    test_x1 = train_x[val_idx]\n",
    "    test_y1 = train_y.loc[val_idx]\n",
    "    #,(vali_x,vali_y)\n",
    "    clf.fit(train_x1, train_y1,eval_set=[(train_x1, train_y1),(test_x1, test_y1)],eval_metric='auc',early_stopping_rounds=100)\n",
    "    res['pred'] += clf.predict_proba(test_x)[:,1]\n",
    "\n",
    "res['pred'] = res['pred']/5\n",
    "res.to_csv('./basesubmission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python37064bitbasecondaf53a82d0388c4e1980d5990867e8724b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}